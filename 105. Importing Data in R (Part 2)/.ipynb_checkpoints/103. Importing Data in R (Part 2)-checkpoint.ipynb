{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing data from databases (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to a database - Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Establish a connection\n",
    "100xp\n",
    "The first step to import data from a SQL database is creating a connection to it. As Filip explained, you need different packages depending on the database you want to connect to. All of these packages do this in a uniform way, as specified in the DBI package.\n",
    "\n",
    "dbConnect() creates a connection between your R session and a SQL database. The first argument has to be a DBIdriver object, that specifies how connections are made and how data is mapped between R and the database. Specifically for MySQL databases, you can build such a driver with RMySQL::MySQL().\n",
    "\n",
    "If the MySQL database is a remote database hosted on a server, you'll also have to specify the following arguments in dbConnect(): dbname, host, port, user and password. Most of these details have already been provided.\n",
    "\n",
    "Instructions\n",
    "Load the DBI library, which is already installed on DataCamp's servers.\n",
    "Edit the dbConnect() call to connect to the MySQL database. Change the port argument (3306) and user argument (\"student\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the DBI package\n",
    "library(DBI)\n",
    "\n",
    "# Edit dbConnect() call\n",
    "con <- dbConnect(RMySQL::MySQL(), \n",
    "                 dbname = \"tweater\", \n",
    "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\", \n",
    "                 port = 3306,\n",
    "                 user = \"student\",\n",
    "                 password = \"datacamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "You are now connected to the MySQL database tweater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Inspect the connection\n",
    "50xp\n",
    "Now that you've successfully created the database connection, let's have a closer look at it. With the object con available in your workspace, can you tell which of the following statements is true?\n",
    "\n",
    "Possible Answers\n",
    "Click or Press Ctrl+1 to focus\n",
    "con is an SQLConnection object.\n",
    "con is a PostgreSQLConnection object.\n",
    "con is an MySQLConnection object. (Correct)\n",
    "con is an NoSQLConnection object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import table data - Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "List the database tables\n",
    "100xp\n",
    "After you've successfully connected to a remote MySQL database, the next step is to see what tables the database contains. You can do this with the dbListTables() function. As you might remember from the video, this function requires the connection object as an input, and outputs a character vector with the table names.\n",
    "\n",
    "Instructions\n",
    "Add code to create a vector tables, that contains the tables in the tweater database. You can connect to this database through the con object.\n",
    "Display the structure of tables; what's the class of this vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the DBI package\n",
    "library(DBI)\n",
    "\n",
    "# Connect to the MySQL database: con\n",
    "con <- dbConnect(RMySQL::MySQL(), \n",
    "                 dbname = \"tweater\", \n",
    "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\", \n",
    "                 port = 3306,\n",
    "                 user = \"student\",\n",
    "                 password = \"datacamp\")\n",
    "\n",
    "# Build a vector of table names: tables\n",
    "tables = dbListTables(con) \n",
    "\n",
    "# Display structure of tables\n",
    "str(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! dbListTables() can be very useful to get a first idea about the contents of your database. Can you guess what kind of information this database contains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Import users\n",
    "100xp\n",
    "As you might have guessed by now, the database contains data on a more tasty version of Twitter, namely Tweater. Users can post tweats with short recipes for delicious snacks. People can comment on these tweats. There are three tables: users, tweats, and comments that have relations among them. Which ones, you ask? You'll discover in a moment!\n",
    "\n",
    "Let's start by importing the data on the users into your R session. You do this with the dbReadTable() function. Simply pass it the connection object (con), followed by the name of the table you want to import. The resulting object is a standard R data frame.\n",
    "\n",
    "Instructions\n",
    "Add code that imports the \"users\" table from the tweater database and store the resulting data frame as users.\n",
    "Print the users data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the DBI package\n",
    "library(DBI)\n",
    "\n",
    "# Connect to the MySQL database: con\n",
    "con <- dbConnect(RMySQL::MySQL(), \n",
    "                 dbname = \"tweater\", \n",
    "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\", \n",
    "                 port = 3306,\n",
    "                 user = \"student\",\n",
    "                 password = \"datacamp\")\n",
    "\n",
    "# Import the users table from tweater: users\n",
    "users = dbReadTable(con, \"users\")\n",
    "\n",
    "# Print users\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Import all tables\n",
    "100xp\n",
    "Next to the users, we're also interested in the tweats and comments tables. However, separate dbReadTable() calls for each and every one of the tables in your database would mean a lot of code duplication. Remember about the lapply() function? You can use it again here! A connection is already coded for you, as well as a vector table_names, containing the names of all the tables in the database.\n",
    "\n",
    "Instructions\n",
    "Finish the lapply() function to import the users, tweats and comments tables in a single call. The result, a list of data frames, will be stored in the variable tables.\n",
    "Print tables to check if you got it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the DBI package\n",
    "library(DBI)\n",
    "\n",
    "# Connect to the MySQL database: con\n",
    "con <- dbConnect(RMySQL::MySQL(), \n",
    "                 dbname = \"tweater\", \n",
    "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\", \n",
    "                 port = 3306,\n",
    "                 user = \"student\",\n",
    "                 password = \"datacamp\")\n",
    "\n",
    "# Get table names\n",
    "table_names <- dbListTables(con)\n",
    "\n",
    "# Import all tables\n",
    "tables <- lapply(table_names, dbReadTable, conn = con)\n",
    "\n",
    "# Print out tables\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have an R version of all data that is contained in the database, you can dive a little deeper into the relations between the different data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "How do the tables relate?\n",
    "50xp\n",
    "The connection to the MySQL database con has already been created for you. tables, a list containing the three tables as data frames that you've created in the previous exercise, is also available.\n",
    "\n",
    "If you have a closer look at these tables, you'll see that the tweats table, for example, contains a column user_id. The ids in the column refer to the users that have posted the tweat. Similarly, the comments contain both a user_id and a tweat_id column. It specifies which user posted a comment on which tweat.\n",
    "\n",
    "With this new knowledge, can you tell who posted the tweat on which somebody commented \"awesome! thanks!\" (comment 1012)?\n",
    "\n",
    "Possible Answers\n",
    "The user with id 1, so Kate.\n",
    "There is not enough information to solve this.\n",
    "The user with id 4, so Thomas.\n",
    "The user with user_id 5, so Oliver. (Correct)\n",
    "Take Hint (-15xp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing data from databases (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Queries from inside R - Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Query tweater (1)\n",
    "100xp\n",
    "In your life as a data scientist, you'll often be working with huge databases that contain tables with millions of rows. If you want to do some analyses on this data, it's possible that you only need a fraction of this data. In this case, it's a good idea to send SQL queries to your database, and only import the data you actually need into R.\n",
    "\n",
    "dbGetQuery() is what you need. As usual, you first pass the connection object to it. The second argument is an SQL query in the form of a character string. This example selects the age variable from the people dataset where gender equals \"male\":\n",
    "\n",
    "dbGetQuery(con, \"SELECT age FROM people WHERE gender = 'male'\")\n",
    "A connection to the tweater database has already been coded for you.\n",
    "\n",
    "Instructions\n",
    "Use dbGetQuery() to create a data frame, elisabeth, that selects the tweat_id column from the comments table where elisabeth is the commenter, her user_id is 1\n",
    "Print out elisabeth so you can see if you queried the database correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "library(DBI)\n",
    "con <- dbConnect(RMySQL::MySQL(),\n",
    "                 dbname = \"tweater\",\n",
    "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
    "                 port = 3306,\n",
    "                 user = \"student\",\n",
    "                 password = \"datacamp\")\n",
    "\n",
    "# Import tweat_id column of comments where user_id is 1: elisabeth\n",
    "elisabeth = dbGetQuery(con, \"SELECT tweat_id FROM comments WHERE user_id = 1\")\n",
    "\n",
    "# Print elisabeth\n",
    "print(elisabeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "To make sure you understood SQL's SELECT - FROM - WHERE syntax, let's practice some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Query tweater (2)\n",
    "100xp\n",
    "Apart from checking equality, you can also check for less than and greater than relationships, with < and >, just like in R.\n",
    "\n",
    "con, a connection to the tweater database, is again available.\n",
    "\n",
    "Instructions\n",
    "Create a data frame, latest, that selects the post column from the tweats table observations where the date is higher than '2015-09-21'.\n",
    "Print out latest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "library(DBI)\n",
    "con <- dbConnect(RMySQL::MySQL(),\n",
    "                 dbname = \"tweater\",\n",
    "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
    "                 port = 3306,\n",
    "                 user = \"student\",\n",
    "                 password = \"datacamp\")\n",
    "\n",
    "# Import post column of tweats where date is higher than '2015-09-21': latest\n",
    "latest = dbGetQuery(con, \"SELECT post FROM tweats WHERE date > '2015-09-21'\")\n",
    "\n",
    "# Print latest\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "To make sure you understood SQL's SELECT - FROM - WHERE syntax, let's practice some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Query tweater (3)\n",
    "100xp\n",
    "Suppose that you have a people table, with a bunch of information. This time, you want to find out the age and country of married males. Provided that there is a married column that's 1 when the person in question is married, the following query would work.\n",
    "\n",
    "SELECT age, country\n",
    "  FROM people\n",
    "    WHERE gender = \"male\" AND married = 1\n",
    "Can you use a similar approach for a more specialized query on the tweater database?\n",
    "\n",
    "Instructions\n",
    "Create an R data frame, specific, that selects the message column from the comments table where the tweat_id is 77 and the user_id is greater than 4.\n",
    "Print specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "library(DBI)\n",
    "con <- dbConnect(RMySQL::MySQL(),\n",
    "                 dbname = \"tweater\",\n",
    "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
    "                 port = 3306,\n",
    "                 user = \"student\",\n",
    "                 password = \"datacamp\")\n",
    "\n",
    "# Create data frame specific\n",
    "specific = dbGetQuery(con, \"SELECT message FROM comments WHERE tweat_id = 77 AND user_id  >4\")\n",
    "\n",
    "# Print specific\n",
    "print(specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Query tweater (4)\n",
    "100xp\n",
    "There are also dedicated SQL functions that you can use in the WHERE clause of an SQL query. For example, CHAR_LENGTH() returns the number of characters in a string.\n",
    "\n",
    "Instructions\n",
    "Create a data frame, short, that selects the id and name columns from the users table where the number of characters in the name is strictly less than 5.\n",
    "Print short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "library(DBI)\n",
    "con <- dbConnect(RMySQL::MySQL(),\n",
    "                 dbname = \"tweater\",\n",
    "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
    "                 port = 3306,\n",
    "                 user = \"student\",\n",
    "                 password = \"datacamp\")\n",
    "\n",
    "# Create data frame short\n",
    "short = dbGetQuery(con, \"SELECT id, name FROM users WHERE CHAR_LENGTH(name) < 5\")\n",
    "\n",
    "# Print short\n",
    "print(short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Join the query madness!\n",
    "50xp\n",
    "Of course, SQL does not stop with the the three keywords SELECT, FROM and WHERE. Another very often used keyword is JOIN, and more specifically INNER JOIN. Take this call for example:\n",
    "\n",
    "SELECT name, post\n",
    "  FROM users INNER JOIN tweats on users.id = user_id\n",
    "    WHERE date > \"2015-09-19\"\n",
    "Here, the users table is joined with the tweats table. This is possible because the id column in the users table corresponds to the user_id column in the tweats table. Also notice how name, from the users table, and post and date, from the tweats table, can be referenced to without problems.\n",
    "\n",
    "Can you predict the outcome of the following query?\n",
    "\n",
    "SELECT post, message\n",
    "  FROM tweats INNER JOIN comments on tweats.id = tweat_id\n",
    "    WHERE tweat_id = 77\n",
    "A connection to the tweater database is already available as con; feel free to experiment!\n",
    "\n",
    "Possible Answers\n",
    "Trying to get the results of this SQL query throws an error.\n",
    "A table with four observations, containing two columns: post and message. (Correct)\n",
    "A table with six observations, containing all columns in the tweats table.\n",
    "A table with six observations, containing the columns post and message.\n",
    "Take Hint (-15xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> dbGetQuery(con, \"SELECT post, message\n",
    "+   FROM tweats INNER JOIN comments on tweats.id = tweat_id\n",
    "+     WHERE tweat_id = 77\")\n",
    "                                           post            message\n",
    "1 2 slices of bread. add cheese. grill. heaven.             great!\n",
    "2 2 slices of bread. add cheese. grill. heaven.      not my thing!\n",
    "3 2 slices of bread. add cheese. grill. heaven. couldn't be better\n",
    "4 2 slices of bread. add cheese. grill. heaven.       saved my day\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Have you tried running this query with dbGetQuery()? The result contains data both from the tweats and the comments column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBI internals - Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Send - Fetch - Clear\n",
    "100xp\n",
    "You've used dbGetQuery() multiple times now. This is a virtual function from the DBI package, but is actually implemented by the RMySQL package. Behind the scenes, the following steps are performed:\n",
    "\n",
    "Sending the specified query with dbSendQuery();\n",
    "Fetching the result of executing the query on the database with dbFetch();\n",
    "Clearing the result with dbClearResult().\n",
    "Let's not use dbGetQuery() this time and implement the steps above. This is tedious to write, but it gives you the ability to fetch the query's result in chunks rather than all at once. You can do this by specifying the n argument inside dbFetch().\n",
    "\n",
    "Instructions\n",
    "Inspect the dbSendQuery() call that has already been coded for you. It selects the comments for the users with an id above 4.\n",
    "Use dbFetch() twice. In the first call, import only two records of the query result by setting the n argument to 2. In the second call, import all remaining queries (don't specify n). In both calls, simply print the resulting data frames.\n",
    "Clear res with dbClearResult()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "library(DBI)\n",
    "con <- dbConnect(RMySQL::MySQL(),\n",
    "                 dbname = \"tweater\",\n",
    "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
    "                 port = 3306,\n",
    "                 user = \"student\",\n",
    "                 password = \"datacamp\")\n",
    "\n",
    "# Send query to the database\n",
    "res <- dbSendQuery(con, \"SELECT * FROM comments WHERE user_id > 4\")\n",
    "\n",
    "# Use dbFetch() twice\n",
    "print(dbFetch(res, 2))\n",
    "print(dbFetch(res))\n",
    "\n",
    "# Clear res\n",
    "dbClearResult(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our toy example, chopping up the fetches doesn't make a lot of sense, but make sure to remember this technique when you're struggling with huge databases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Be polite and ...\n",
    "100xp\n",
    "Every time you connect to a database using dbConnect(), you're creating a new connection to the database you're referencing. RMySQL automatically specifies a maximum of open connections and closes some of the connections for you, but still: it's always polite to manually disconnect from the database afterwards. You do this with the dbDisconnect() function.\n",
    "\n",
    "The code that connects you to the database is already available, can you finish the script?\n",
    "\n",
    "Instructions\n",
    "Using the technique you prefer, build a data frame long_tweats. It selects the post and date columns from the observations in tweats where the character length of the post variable exceeds 40.\n",
    "Print long_tweats.\n",
    "Disconnect from the database by using dbDisconnect()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load RMySQL package\n",
    "library(DBI)\n",
    "\n",
    "# Connect to the database\n",
    "library(DBI)\n",
    "con <- dbConnect(RMySQL::MySQL(),\n",
    "                 dbname = \"tweater\",\n",
    "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
    "                 port = 3306,\n",
    "                 user = \"student\",\n",
    "                 password = \"datacamp\")\n",
    "\n",
    "# Create the data frame  long_tweats\n",
    "long_tweats = dbGetQuery(con, \"SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) > 40\")\n",
    "\n",
    "# Print long_tweats\n",
    "print(long_tweats)\n",
    "\n",
    "# Disconnect from the database\n",
    "dbDisconnect(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the chapter on databases. Of course, there is tons more to learn about interfacing to databases and working with them as efficiently as possible, but that's something for more advanced courses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Importing data from the web (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP - Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Import flat files from the web\n",
    "100xp\n",
    "In the video, you saw that the utils functions to import flat file data, such as read.csv() and read.delim(), are capable of automatically importing from URLs that point to flat files on the web.\n",
    "\n",
    "You must be wondering whether Hadley Wickham's alternative package, readr, is equally potent. Well, figure it out in this exercise! The URLs for both a .csv file as well as a .delim file are already coded for you. It's up to you to actually import the data. If it works, that is...\n",
    "\n",
    "Instructions\n",
    "Load the readr package. It's already installed on DataCamp's servers.\n",
    "Use url_csv to read in the .csv file it is pointing to. Use the read_csv() function. The .csv contains column names in the first row. Save the resulting data frame as pools.\n",
    "Similarly, use url_delim to read in the online .txt file. Use the read_tsv() function and store the result as potatoes.\n",
    "Print pools and potatoes. Looks correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the readr package\n",
    "library(readr)\n",
    "\n",
    "# Import the csv file: pools\n",
    "url_csv <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv\"\n",
    "pools = read_csv(url_csv)\n",
    "\n",
    "# Import the txt file: potatoes\n",
    "url_delim <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt\"\n",
    "potatoes = read_tsv(url_delim)\n",
    "\n",
    "# Print pools and potatoes\n",
    "print(pools)\n",
    "print(potatoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Secure importing\n",
    "100xp\n",
    "In the previous exercises, you have been working with URLs that all start with http://. There is, however, a safer alternative to HTTP, namely HTTPS, which stands for HypterText Transfer Protocol Secure. Just remember this: HTTPS is relatively safe, HTTP is not.\n",
    "\n",
    "Luckily for us, you can use the standard importing functions with https:// connections since R version 3.2.2.\n",
    "\n",
    "Instructions\n",
    "Take a look at the URL in url_csv. It uses a secure connection, https://.\n",
    "Use read.csv() to import the file at url_csv. The .csv file it is referring to contains column names in the first row. Call it pools1.\n",
    "Load the readr package. It's already installed on DataCamp's servers.\n",
    "Use read_csv() to read in the same .csv file in url_csv. Call it pools2.\n",
    "Print out the structure of pools1 and pools2. Looks like the importing went equally well as with a normal http connection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https URL to the swimming_pools csv file.\n",
    "url_csv <- \"https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv\"\n",
    "\n",
    "# Import the file using read.csv(): pools1\n",
    "pools1 = read.csv(url_csv)\n",
    "\n",
    "# Load the readr package\n",
    "library(readr)\n",
    "\n",
    "# Import the file using read_csv(): pools2\n",
    "pools2 = read_csv(url_csv)\n",
    "\n",
    "# Print the structure of pools1 and pools2\n",
    "str(pools1)\n",
    "str(pools2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading files - Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Import Excel files from the web\n",
    "100xp\n",
    "When you learned about gdata, it was already mentioned that gdata can handle .xls files that are on the internet. readxl can't, at least not yet. The URL with which you'll be working is already available in the sample code. You will import it once using gdata and once with the readxl package via a workaround.\n",
    "\n",
    "Instructions\n",
    "Load the readxl and gdata packages. They are already installed on DataCamp's servers.\n",
    "Import the .xls file located at the URL url_xls using read.xls() from gdata. Store the resulting data frame as excel_gdata.\n",
    "You can not use read_excel() directly with a URL. Complete the following instructions to work around this problem:\n",
    "Use download.file() to download the .xls file behind the URL and store it locally as \"local_latitude.xls\".\n",
    "Call read_excel() to import the local file, \"local_latitude.xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the readxl and gdata package\n",
    "library(readxl)\n",
    "library(gdata)\n",
    "\n",
    "\n",
    "# Specification of url: url_xls\n",
    "url_xls <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls\"\n",
    "\n",
    "# Import the .xls file with gdata: excel_gdata\n",
    "excel_gdata = read.xls(url_xls)\n",
    "\n",
    "# Download file behind URL, name it local_latitude.xls\n",
    "download.file(url_xls, \"local_latitude.xls\")\n",
    "\n",
    "# Import the local .xls file with readxl: excel_readxl\n",
    "excel_readxl = read_excel(\"local_latitude.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It appears that readxl is not (yet?) able to deal with Excel files that are on the web. However, a simply workaround with download.file() fixes this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Downloading any file, secure or not\n",
    "100xp\n",
    "In the previous exercise you've seen how you can read excel files on the web using the read_excel package by first downloading the file with the download.file() function.\n",
    "\n",
    "There's more: with download.file() you can download any kind of file from the web, using HTTP and HTTPS: images, executable files, but also .RData files. An RData file is very efficient format to store R data.\n",
    "\n",
    "You can load data from an RData file using the load() function, but this function does not accept a URL string as an argument. In this exercise, you'll first download the RData file securely, and then import the local data file.\n",
    "\n",
    "Instructions\n",
    "Take a look at the URL in url_rdata. It uses a secure connection, https://. This URL points to an RData file containing a data frame with some metrics on different kinds of wine.\n",
    "Download the file at url_rdata using download.file(). Call the file \"wine_local.RData\" in your working directory.\n",
    "Load the file you created, wine_local.RData, using the load() function. It takes one argument, the path to the file, which is just the filename in our case. After running this command, the variable wine will automatically be available in your workspace.\n",
    "Print out the summary() of the wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https URL to the wine RData file.\n",
    "url_rdata <- \"https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData\"\n",
    "\n",
    "# Download the wine file to your working directory\n",
    "download.file(url_rdata,\"wine_local.RData\")\n",
    "\n",
    "# Load the wine data into your workspace using load()\n",
    "load(\"wine_local.RData\")\n",
    "\n",
    "# Print out the summary of the wine data\n",
    "summary(wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to load remote RData files is to use the url() function inside load(). However, this will not save the RData file to a local file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a text file from the web\n",
    "50xp\n",
    "Wow, you've learned a lot of ways to import a data file from the web in the previous exercises. Let's see if you can remember what's possible and what's not.\n",
    "\n",
    "Which way of importing data is NOT possible?\n",
    "\n",
    "Possible Answers\n",
    "Importing a .csv file residing on the web using the URL with read.csv(): read.csv(\"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv\")\n",
    "    \n",
    "Downloading a remote excel and saving it to your working directory using download.file(): download.file(\"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xlsx\", \"lat.xlsx\")\n",
    "\n",
    "Importing a .txt file residing on the web using the URL with read_tsv(): \n",
    "read_tsv(\"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt\") \n",
    "\n",
    "Using the load() function to load a remote RData file into the workspace with only the URL string:load(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData\") (Incorrect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't directly use a URL string inside load() to load remote RData files. You should use url() or download the file first using download.file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTTP? httr! (1)\n",
    "100xp\n",
    "Downloading a file from the Internet means sending a GET request and receiving the file you asked for. Internally, all the previously discussed functions use a GET request to download files.\n",
    "\n",
    "httr provides a convenient function, GET() to execute this GET request. The result is a response object, that provides easy access to the status code, content-type and, of course, the actual content.\n",
    "\n",
    "You can extract the content from the request using the content() function. At the time of writing, there are three ways to retrieve this content: as a raw object, as a character vector, or an R object, such as a list. If you don't tell content() how to retrieve the content through the as argument, it'll try its best to figure out which type is most appropriate based on the content-type.\n",
    "\n",
    "Instructions\n",
    "Load the httr package. It's already installed on DataCamp's servers.\n",
    "Use GET() to get the URL stored in url. Store the result of this GET() call as resp.\n",
    "Print the resp object. What information does it contain?\n",
    "Get the content of resp using content() and set the as argument to \"raw\". Assign the resulting vector to raw_content.\n",
    "Print the first values in raw_content with head()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the httr package\n",
    "library(httr)\n",
    "\n",
    "# Get the url, save response to resp\n",
    "url <- \"http://www.example.com/\"\n",
    "resp = GET(url)\n",
    "\n",
    "# Print resp\n",
    "print(resp)\n",
    "\n",
    "# Get the raw content of resp: raw_content\n",
    "raw_content = content(resp, as = \"raw\")\n",
    "\n",
    "# Print the head of raw_content\n",
    "print(head(raw_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw content of the response doesn't make a lot of sense, does it? Luckily, the content() function by default, if you don't specify the as argument, figures out what type of data you're dealing with and parses it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTTP? httr! (2)\n",
    "100xp\n",
    "Web content does not limit itself to HTML pages and files stored on remote servers such as DataCamp's Amazon S3 instances. There are many other data formats out there. A very common one is JSON. This format is very often used by so-called Web APIs, interfaces to web servers with which you as a client can communicate to get or store information in more complicated ways.\n",
    "\n",
    "You'll learn about Web APIs and JSON in the video and exercises that follow, but some experimentation never hurts, does it?\n",
    "\n",
    "Instructions\n",
    "Use GET() to get the url that has already been specified in the sample code. Store the response as resp.\n",
    "Print resp. What is the content-type?\n",
    "Use content() to get the content of resp. Set the as argument to \"text\". Simply print out the result. What do you see?\n",
    "Use content() to get the content of resp, but this time do not specify a second argument. R figures out automatically that you're dealing with a JSON, and converts the JSON to a named R list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# httr is already loaded\n",
    "\n",
    "# Get the url\n",
    "url <- \"http://www.omdbapi.com/?apikey=ff21610b&t=Annie+Hall&y=&plot=short&r=json\"\n",
    "resp = GET(url)\n",
    "\n",
    "# Print resp\n",
    "print(resp)\n",
    "\n",
    "# Print content of resp as text\n",
    "print(content(resp, as = \"text\"))\n",
    "\n",
    "# Print content of resp\n",
    "print(content(resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that httr converts the JSON response body automatically to an R list is very convenient.\n",
    "\n",
    "You have finished the chapter \"Importing data from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
